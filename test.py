import os
import gc
import h5py
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import argparse
from sklearn.metrics import roc_curve, auc

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹
from model import GCN_CSS, CNN_CSS, MLP_CSS 

# ================= é»˜è®¤é…ç½® =================
# è¿™é‡Œçš„è·¯å¾„è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼Œå¦‚æœå’Œ main.py ä¼ å‚ä¸€è‡´ä¹Ÿå¯ä»¥
DEFAULT_HDF5_PATH = 'GOLD_XYZ_OSC.0001_1024.hdf5' 
BATCH_SIZE = 32  
NUM_NODES = 32
TARGET_PFA = 0.1 
SAMPLES_PER_SNR = 100 # æ¯ä¸ª SNR ç‚¹é‡‡æ ·çš„æ ·æœ¬æ•°

# å¼ºåˆ¶ä½¿ç”¨ CPU è¿›è¡Œæ¨ç† (é¿å… GPU æ˜¾å­˜å†²çªï¼Œä¸”è¯„ä¼°æ•°æ®é‡ä¸å¤§ï¼ŒCPU è¶³å¤Ÿ)
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

def parse_args():
    parser = argparse.ArgumentParser(description="Evaluate Trained Models")
    parser.add_argument('--path', type=str, default=DEFAULT_HDF5_PATH, help='Path to .hdf5 dataset')
    parser.add_argument('--antennas', type=int, default=1, help='Number of antennas M (used during training)')
    parser.add_argument('--model_type', type=str, default='all', choices=['all', 'gcn', 'cnn', 'mlp'], help='Model type to evaluate')
    return parser.parse_args()

# ================= æ•°æ®åŠ è½½ =================
def load_test_data(hdf5_path, samples_per_snr=100, num_antennas=1):
    print(f"ğŸš€ æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ® (M={num_antennas}, æ¯SNRé‡‡æ ·={samples_per_snr})...")
    
    if not os.path.exists(hdf5_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ•°æ®é›†: {hdf5_path}")

    with h5py.File(hdf5_path, 'r') as f:
        Z_all = f['Z'][:]
        unique_snrs = np.unique(Z_all)
        
        selected_indices = []
        np.random.seed(2024)
        
        # 1. æŒ‰ SNR é‡‡æ ·
        for snr in unique_snrs:
            indices = np.where(Z_all == snr)[0]
            if len(indices) > samples_per_snr:
                chosen = np.random.choice(indices, samples_per_snr, replace=False)
            else:
                chosen = indices
            selected_indices.extend(chosen)
        selected_indices = np.sort(np.array(selected_indices))
        
        # 2. è¯»å–ä¿¡å·æ•°æ® X
        # ä¸ºäº†é¿å…å†…å­˜çˆ†ç‚¸ï¼Œåˆ†å—è¯»å–
        X_chunks = []
        chunk_size = 2000 
        for i in range(0, len(selected_indices), chunk_size):
            subset = selected_indices[i : i + chunk_size]
            X_chunks.append(f['X'][subset])
        
        X_sig = np.concatenate(X_chunks, axis=0)
        Z_sig = Z_all[selected_indices]
        
        # 3. ä¼°ç®—åº•å™ª (ç”¨äºç”Ÿæˆ H0)
        # æ‰¾ -20dB æˆ–æœ€å° SNR æ ·æœ¬
        noise_ref_snr = -20
        noise_idx = np.where(Z_all == noise_ref_snr)[0]
        if len(noise_idx) == 0:
            noise_ref_snr = np.min(Z_all)
            noise_idx = np.where(Z_all == noise_ref_snr)[0]
            
        # é‡‡æ ·è®¡ç®— Std
        limit = min(2000, len(noise_idx))
        X_floor = f['X'][noise_idx[:limit]]
        noise_std = np.std(X_floor)
        print(f"ğŸ“‰ ä¼°è®¡çš„ç‰©ç†åº•å™ª ({noise_ref_snr}dB): Std={noise_std:.6f}")

    # 4. ç”Ÿæˆçº¯å™ªå£°æ ·æœ¬ H0
    X_noise = np.random.normal(0, noise_std, size=X_sig.shape).astype(np.float32)
    Z_noise = np.full((len(X_sig), 1), -100.0)
    
    # åˆå¹¶
    X = np.concatenate([X_noise, X_sig], axis=0)
    Y = np.concatenate([np.zeros(len(X_sig)), np.ones(len(X_sig))])
    Z = np.concatenate([Z_noise, Z_sig])
    
    print(f"âœ… æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæ¯•: {X.shape}")
    return X, Y, Z.flatten(), noise_std

# ================= æ‰¹å¤„ç†ä¸å¤šå¤©çº¿æ¨¡æ‹Ÿ =================
def process_batch(X_raw, num_antennas=1, is_gcn=True):
    # X_raw: (Batch, 1024, 2) -> éœ€è¦ reshape æˆ (Batch, Nodes, Base_Feats)
    # Base_Feats = 1024*2 / 32 = 64
    base_feat_dim = 1024 * 2 // NUM_NODES
    X_r = X_raw.reshape(-1, NUM_NODES, base_feat_dim)
    
    # ã€æ ¸å¿ƒã€‘å¤šå¤©çº¿æ‰©å±• (MIMO Simulation)
    # å¿…é¡»ä¸ dataset.py ä¸­çš„é€»è¾‘ä¿æŒä¸€è‡´ï¼šä½¿ç”¨ np.tile å¤åˆ¶
    if num_antennas > 1:
        X_r = np.tile(X_r, (1, 1, num_antennas))
        # æµ‹è¯•æ—¶é€šå¸¸ä¸åŠ éšæœºæ‰°åŠ¨ï¼Œä»¥ä¿è¯ç»“æœç¡®å®šæ€§
    
    X_t = tf.convert_to_tensor(X_r, dtype=tf.float32)
    
    if is_gcn:
        # GCN è®¡ç®—é‚»æ¥çŸ©é˜µ
        # ä½¿ç”¨æ‰©å±•åçš„ç‰¹å¾è®¡ç®—æ¬§æ°è·ç¦»
        diff = tf.expand_dims(X_t, 2) - tf.expand_dims(X_t, 1)
        dist = tf.reduce_sum(tf.square(diff), axis=-1)
        A = tf.exp(-dist) 
        D = tf.reduce_sum(A, axis=-1, keepdims=True)
        A = A / (D + 1e-6)
        return [X_t, A]
    else:
        # CNN/MLP ä¸éœ€è¦ Aï¼Œä¼ ä¸ªå ä½ç¬¦
        batch_size = tf.shape(X_t)[0]
        dummy = tf.zeros((batch_size, 1), dtype=tf.float32)
        return [X_t, dummy]

# ================= æ¨ç†ä¸»å‡½æ•° =================
def run_evaluation(model_class, model_path, model_name, X, M):
    print(f"\nğŸ¤– æ­£åœ¨è¯„ä¼°æ¨¡å‹: {model_name} (M={M})...")
    print(f"   æƒé‡è·¯å¾„: {model_path}")
    
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æƒé‡æ–‡ä»¶ {model_path}")
        return None

    # æ¸…ç†å†…å­˜
    tf.keras.backend.clear_session()
    gc.collect()
    
    # å®ä¾‹åŒ–æ¨¡å‹
    # æ³¨æ„ï¼šæ‰€æœ‰æ¨¡å‹ç±»éƒ½éœ€è¦æ¥æ”¶ (num_classes, num_nodes)
    model = model_class(2, NUM_NODES)
    
    # Build æ¨¡å‹ä»¥åŠ è½½æƒé‡
    # è¾“å…¥ç‰¹å¾ç»´æ•° = 64 * M
    base_dim = 64
    total_dim = base_dim * M
    
    try:
        # æ˜¾å¼ buildï¼Œç¡®ä¿å½¢çŠ¶åŒ¹é…
        model.build([(None, NUM_NODES, total_dim), (None, NUM_NODES, NUM_NODES)])
        model.load_weights(model_path)
    except Exception as e:
        print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
        print("   æç¤º: è¯·æ£€æŸ¥æ¨¡å‹ç»“æ„æ˜¯å¦ä¸è®­ç»ƒæ—¶ä¸€è‡´ (model.py)")
        return None

    # é¢„æµ‹å¾ªç¯
    preds = []
    total = len(X)
    is_gcn = 'gcn' in model_name.lower() or 'proposed' in model_name.lower()
    
    for i in range(0, total, BATCH_SIZE):
        bx = X[i : i+BATCH_SIZE]
        inputs = process_batch(bx, num_antennas=M, is_gcn=is_gcn)
        
        # é¢„æµ‹ (è¿”å› Softmax æ¦‚ç‡)
        p = model.predict_on_batch(inputs)
        # å–ç±»åˆ« 1 (ä¿¡å·å­˜åœ¨) çš„æ¦‚ç‡
        preds.append(p[:, 1])
        
        if i % (BATCH_SIZE * 50) == 0:
            print(f"   è¿›åº¦: {i}/{total}", end='\r')
            
    print(f"   è¿›åº¦: {total}/{total} [å®Œæˆ]")
    return np.concatenate(preds)

# ================= ç»˜å›¾ =================
def plot_results(results_dict, Y_true, Z_snr, M):
    suffix = f"_m{M}"
    
    # 1. Pd vs SNR
    plt.figure(figsize=(10, 6))
    snr_range = np.arange(-20, 31, 2)
    colors = {'GCN': 'red', 'CNN': 'blue', 'MLP': 'green'}
    markers = {'GCN': 'o', 'CNN': 's', 'MLP': '^'}
    
    for name, scores in results_dict.items():
        # ç¡®å®šæ ·å¼
        c = 'black'
        m = 'x'
        for k in colors:
            if k in name.upper(): 
                c = colors[k]
                m = markers[k]
                
        # è®¡ç®—è™šè­¦é˜ˆå€¼
        noise_scores = scores[Y_true == 0]
        thresh = np.percentile(noise_scores, (1 - TARGET_PFA)*100)
        
        pd_list = []
        for snr in snr_range:
            # æ‰¾ç‰¹å®š SNR çš„ä¿¡å·æ ·æœ¬
            idx = np.where((Y_true == 1) & (np.abs(Z_snr - snr) < 1.0))[0]
            if len(idx) == 0:
                pd_list.append(0)
            else:
                pd = np.mean(scores[idx] > thresh)
                pd_list.append(pd)
        
        plt.plot(snr_range, pd_list, label=name, color=c, marker=m)
        
    plt.title(f'Detection Probability vs SNR (M={M}, Pfa={TARGET_PFA})')
    plt.xlabel('SNR (dB)')
    plt.ylabel('Pd')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.ylim([0, 1.05])
    plt.xlim([-20, 30])
    plt.savefig(f'eval_pd_snr{suffix}.png')
    print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜: eval_pd_snr{suffix}.png")

    # 2. ROC Curve at -10dB
    plt.figure(figsize=(8, 8))
    target_snr = -10
    
    # ç­›é€‰ -10dB é™„è¿‘çš„ä¿¡å· + æ‰€æœ‰å™ªå£°
    sig_idx = np.where((Y_true == 1) & (np.abs(Z_snr - target_snr) < 1.0))[0]
    noise_idx = np.where(Y_true == 0)[0]
    
    if len(sig_idx) > 0:
        y_roc = np.concatenate([np.zeros(len(noise_idx)), np.ones(len(sig_idx))])
        
        for name, scores in results_dict.items():
            s_roc = np.concatenate([scores[noise_idx], scores[sig_idx]])
            fpr, tpr, _ = roc_curve(y_roc, s_roc)
            roc_auc = auc(fpr, tpr)
            
            c = 'black'
            for k in colors:
                if k in name.upper(): c = colors[k]
                
            plt.plot(fpr, tpr, label=f"{name} (AUC={roc_auc:.4f})", color=c)
    
    plt.plot([0, 1], [0, 1], 'k--')
    plt.title(f'ROC Curve at {target_snr}dB (M={M})')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.savefig(f'eval_roc{suffix}.png')
    print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜: eval_roc{suffix}.png")

def main():
    args = parse_args()
    
    # 1. åŠ è½½æ•°æ®
    X, Y, Z, _ = load_test_data(args.path, num_antennas=args.antennas)
    
    # 2. å®šä¹‰å¾…è¯„ä¼°æ¨¡å‹
    # è‡ªåŠ¨æ ¹æ® M ç”Ÿæˆæ–‡ä»¶å
    models_to_run = []
    
    if args.model_type in ['all', 'gcn']:
        models_to_run.append({
            'name': f'GCN (M={args.antennas})', 
            'class': GCN_CSS, 
            'path': f'best_gcn_m{args.antennas}.h5'
        })
    if args.model_type in ['all', 'cnn']:
        models_to_run.append({
            'name': f'CNN (M={args.antennas})', 
            'class': CNN_CSS, 
            'path': f'best_cnn_m{args.antennas}.h5'
        })
    if args.model_type in ['all', 'mlp']:
        models_to_run.append({
            'name': f'MLP (M={args.antennas})', 
            'class': MLP_CSS, 
            'path': f'best_mlp_m{args.antennas}.h5'
        })

    # 3. è¿è¡Œè¯„ä¼°
    results = {}
    for m in models_to_run:
        scores = run_evaluation(m['class'], m['path'], m['name'], X, args.antennas)
        if scores is not None:
            results[m['name']] = scores
            
    # 4. ç»˜å›¾
    if len(results) > 0:
        plot_results(results, Y, Z, args.antennas)
    else:
        print("âš ï¸ æ²¡æœ‰å¾—åˆ°ä»»ä½•é¢„æµ‹ç»“æœï¼Œæ— æ³•ç»˜å›¾ã€‚")

if __name__ == "__main__":
    main()